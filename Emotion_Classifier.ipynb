{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzFuQqdv9FN"
      },
      "source": [
        "# Emotion Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQV4bHtRwfJ-"
      },
      "source": [
        "## Instalación de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install mediapipe\n",
        "#%pip install imbalanced-learn\n",
        "#%pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHqjcab3v6Rp"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETE_7Yd-wyPU"
      },
      "source": [
        "## Rutas de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUTA_BASE = \"SAMMv3\"\n",
        "TAMAÑO_FINAL = 224\n",
        "\n",
        "MAPEO_EMOCIONES = {\n",
        "    \"Anger\": 0,\n",
        "    \"Contempt\": 1,\n",
        "    \"Disgust\": 2,\n",
        "    \"Fear\": 3,\n",
        "    \"Happiness\": 4,\n",
        "    \"Sadness\": 5,\n",
        "    \"Surprise\": 6\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZWH_1RShKxa"
      },
      "source": [
        "## Funciones de preprocesamiento de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `cargar_imagen` realiza lo siguiente:\n",
        "1. Carga una imagen y la convierte en un arreglo de numpy donde cada pixel se representa como un valor numérico (esto si la imagen está en escala de grises) o como un conjuntos de valores si la imagen es a color.\n",
        "\n",
        "2. Esta imagen se reduce a la mitad de su tamaño original (1/2 resolución) con la función IMREAD_REDUCED_GRAYSCALE_2.\n",
        "\n",
        "\n",
        "Resultado: Imagen procesada como un arreglo de 2 dimensiones (alto, ancho)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cargar_imagen(ruta):\n",
        "    try:\n",
        "        # IMREAD_REDUCED_GRAYSCALE_2 carga en escala de grises la imagen reducida a su mitad. \n",
        "        # Por ejemplo si se carga una imagen de 500x300 el tamaño final seria de 250x150 px.\n",
        "        imagen = cv2.imread(ruta, cv2.IMREAD_REDUCED_GRAYSCALE_2)\n",
        "        if imagen is None:\n",
        "            return None\n",
        "        return imagen\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `recortar_imagen` realiza lo siguiente:\n",
        "1. Recorta la imagen cargada a un cuadrado y luego la redimensiona al TAMAÑO_FINAL (224x224 px)\n",
        "\n",
        "Resultado: Imagen en 224x224 px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recortar_imagen(imagen):\n",
        "    try:\n",
        "        alto, ancho = imagen.shape[:2] # obtener el alto y ancho de la imagen\n",
        "        # se toma el lado mas pequeño para recortar un cuadro centrado en la imagen\n",
        "        lado = min(alto, ancho)\n",
        "        y = (alto - lado) // 2\n",
        "        x = (ancho - lado) // 2\n",
        "        recorte = imagen[y:y+lado, x:x+lado]\n",
        "        # se redimensiona la imagen recortada al tamaño final deseado (224x224 px)\n",
        "        return cv2.resize(recorte, (TAMAÑO_FINAL, TAMAÑO_FINAL))\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `extraer_landmarks` realiza lo siguiente:\n",
        "1. Recibe una imagen y una instancia del mallador de MediaPipe (FaceMesh), que se encarga de detectar puntos clave del rostro (landmarks).\n",
        "2. MediaPipe requiere imágenes a color, por lo que también verifica si la imagen es en escala de grises.\n",
        "\n",
        "La condición `if resultados.multi_face_landmarks` verifica si se detectó al menos un rostro en la imagen, ya que `multi_face_landmarks` es una lista con los resultados de los landmarks de cada cara encontrada, por lo tanto si no se detecta ningún rostro, se regresa un None, pero si se detecta una cara, entonces se toma con `resultados.multi_face_landmarks[0]` (el hecho de que sea índice 0 indica que es el primer rostro detectado).\n",
        "Luego se accede a `.landmark`, esto es una lista de 468 puntos faciales del modelo Face Mesh, donde cada punto tiene coordenadas `(x, y, z)`, aunque en este caso solo se tomaron las coordenadas `x` e `y` para construir un arreglo de numpy de la forma `(468, 2)`, donde el 468 indica que son 468 puntos faciales y el 2, su número de coordenadas por cada punto. De manera que el arreglo tendría la siguiente forma: [[coordenada x1, coordenada y1],[coordenada x2, coordenada y2], ...]\n",
        "\n",
        "Resultado: Un arreglo de numpy con las coordenadas normalizadas (x, y) de los puntos clave del rostro o None, si no se detectó ninguna cara."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extraer_landmarks(imagen, mallador):\n",
        "    try:\n",
        "        if len(imagen.shape) == 2: # verificar si la imagen es en escala de grises. Si lo es, entonces se convierte a RGB\n",
        "            imagen = cv2.cvtColor(imagen, cv2.COLOR_GRAY2RGB) \n",
        "        # procesar la imagen con el mallador de MediaPipe\n",
        "        resultados = mallador.process(imagen)\n",
        "        ########################################################################################\n",
        "        if resultados.multi_face_landmarks: \n",
        "            return np.array([[p.x, p.y] for p in resultados.multi_face_landmarks[0].landmark])\n",
        "        ########################################################################################\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `normalizar_landmarks`\n",
        "El objetivo de este paso es transformar los landmarks para que no se vean afectados por el tamaño del rostro y de su posición en la imagen, por lo tanto lo que recibe esta función es un arreglo de landmarks que tiene la forma (468, 2) y donde cada fila es un punto del rostro con coordenadas normalizadas (x, y) entre 0 y 1 (este arreglo fue la salida de MediaPipe que se proceso con la función `extraer_landmarks`).\n",
        "\n",
        "Por lo tanto, el resultado de aplicar esta función es un vector centrado y escalado de 936 valores que representa la forma del rostro sin importar su posición o tamaño en la imagen que se procese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalizar_landmarks(landmarks):\n",
        "    try:\n",
        "        centro = landmarks[1] # landmarks[1] es un punto fijo del rostro por ejemplo la nariz\n",
        "        landmarks -= centro # se resta el \"centro\" a los demás puntos de landmarks para centrar la cara\n",
        "        escala = np.linalg.norm(landmarks[234] - landmarks[454]) # calcula la distancia entre dos puntos fijos del rostro, en este caso, el landmark 234 \n",
        "        # es la mejilla izquierda y el 454 es la mejilla derecha, esto para obtener el ancho de la cara\n",
        "        return (landmarks / escala).flatten() # .fltten convierte el arreglo de forma (468, 2) a un arreglo de una dimensión (936,), es decir, un vector de 936 valores\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga de datos y balanceo de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `cargar_dataset`:\n",
        "-Se carga el modelo de detección landmarks  con: **mp.solutions.face_mesh** \n",
        "\n",
        "-Se crea el objeto **mallador**, el cual se utilizará para procesar las imágenes y extraer sus landmarks.\n",
        "\n",
        "-Se inicializa un diccionario **datos_por_clase** que almacenará los datos obtenidos por cada emoción.\n",
        "\n",
        "-Luego con **MAPEO_EMOCIONES**, se accede a la carpeta correspondiente con las imágenes por cada emoción.\n",
        "\n",
        "-Se verifica que la carpeta exista y que contenga imágenes en formato .jpg\n",
        "\n",
        "\n",
        "* Si hay más de 900 imágenes, solo se usan solo 900 para mantener un balance entre las clases y luego dentro del bucle **for archivo in tqdm ...** se utilizan las funciones previamente desarrolladas para cargar y procesar cada imagen.\n",
        "\n",
        "* En otro caso, donde haya menos de 800 imágenes, se aplica SMOTE (Synthetic Minority Oversampling Technique) para generar muestras sintéticas hasta llegar a 800 muestras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cargar_dataset():\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    mallador = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3)\n",
        "    # PARAMETROS:\n",
        "        # static_image_mode=True: indica que se procesarán imágenes, no un video.\n",
        "        # max_num_faces=1: es el núm. máximo de rostros que buscará el modelo por cada imagen.\n",
        "        # min_detection_confidence=0.3: solo se detectarán landmarks si el modelo tiene al menos 30% de confianza en que hay un rostro en la imagen.\n",
        "\n",
        "    datos_por_clase = {}\n",
        "\n",
        "    for emocion, codigo in MAPEO_EMOCIONES.items():\n",
        "        carpeta = os.path.join(RUTA_BASE, emocion)\n",
        "        if not os.path.exists(carpeta):\n",
        "            print(f\"Carpeta no encontrada. {carpeta}\")\n",
        "            continue\n",
        "\n",
        "        archivos = [f for f in os.listdir(carpeta) if f.lower().endswith('jpg')]\n",
        "        total = len(archivos)\n",
        "        if total == 0:\n",
        "            continue\n",
        "        # si hay más de 900 imágenes, solo se usan solo 900 para mantener un balance entre las clases.\n",
        "        if total > 900:\n",
        "            archivos = archivos[:900]\n",
        "\n",
        "        print(f\"Procesando '{emocion}' ({len(archivos)} imágenes)...\")\n",
        "        # Se inicializan las listas X_tmp, y_tmp para almacenar los landmarks normalizados y las etiquetas de cada emocion\n",
        "        X_tmp, y_tmp = [], []\n",
        "\n",
        "        for archivo in tqdm(archivos, desc=f\"Procesando {emocion}\"):\n",
        "            ruta = os.path.join(carpeta, archivo)\n",
        "            img = cargar_imagen(ruta)\n",
        "            if img is not None:\n",
        "                img_proc = recortar_imagen(img)\n",
        "                if img_proc is not None:\n",
        "                    landmarks = extraer_landmarks(img_proc, mallador)\n",
        "                    if landmarks is not None:\n",
        "                        norm = normalizar_landmarks(landmarks)\n",
        "                        if norm is not None:\n",
        "                            X_tmp.append(norm)\n",
        "                            y_tmp.append(codigo)\n",
        "\n",
        "        if len(X_tmp) < 800 and len(X_tmp) > 0:\n",
        "            print(f\"Aplicando SMOTE para '{emocion}' ({len(X_tmp)} -> 800)...\")\n",
        "            X_tmp.append(np.zeros_like(X_tmp[0])) \n",
        "            y_tmp.append(-1) \n",
        "            smote = SMOTE(sampling_strategy={codigo: 800}, random_state=42) # se aplica SMOTE para balancear la clase\n",
        "            X_res, y_res = smote.fit_resample(X_tmp, y_tmp)\n",
        "            X_tmp = [x for x, yv in zip(X_res, y_res) if yv == codigo]\n",
        "            y_tmp = [codigo] * len(X_tmp)\n",
        "\n",
        "        datos_por_clase[codigo] = (X_tmp, y_tmp) # vector landmarks norms., etiqueta por cada emocion\n",
        "\n",
        "    mallador.close() # liberar recursos del objeto FaceMesh\n",
        "    X, y = [], []\n",
        "    for Xc, yc in datos_por_clase.values():\n",
        "        X.extend(Xc)\n",
        "        y.extend(yc)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXS1QFcOhYtq"
      },
      "source": [
        "## Entrenar y evaluar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este bloque de código se hace lo siguiente: \n",
        "\n",
        "-Define un pipeline en el cual se usa **StandardScaler()** para estandarizar los vectores de entrada (restar la media y dividir por la desviación estándar), esto se hace porque el modelo SVM es sensible a la escala de los datos.\n",
        "\n",
        "-Luego se aplica un clasificador SVM (Máquinas de Vectores de Soporte) con kernel RBF (Radial Basis Function).\n",
        "\n",
        "-Se configuran los siguientes hiperparámetros:\n",
        "\n",
        "        C=0.5: penalización por errores (más grande = más ajuste al entrenamiento (overfitting)).\n",
        "\n",
        "        gamma='scale': controla la influencia de cada muestra, scale es el valor recomendado por defecto.\n",
        "\n",
        "        class_weight='balanced': ajusta automáticamente el peso de cada clase con base en su frecuencia.\n",
        "\n",
        "Finalmente se entrena el modelo con los datos de X_train, y_train para obtener el modelo que realice las clasificaciones y guardarlo en un pkl con **joblib.dump**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"****CARGANDO DATASET****\")\n",
        "X, y = cargar_dataset()\n",
        "print(f\"\\nTotal de muestras: {len(X)}\\nDistribución final:\", Counter(y)) # verificar el balance entre emociones\n",
        "\n",
        "print(\"\\n****VALIDACIÓN CRUZADA****\")\n",
        "modelo = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=0.5, gamma='scale', class_weight='balanced'))\n",
        "cv = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
        "acc_scores = cross_val_score(modelo, X, y, cv=cv, scoring='accuracy')\n",
        "f1_scores = cross_val_score(modelo, X, y, cv=cv, scoring='f1_macro')\n",
        "print(f\"Accuracy promedio: {acc_scores.mean():.4f}\")\n",
        "print(f\"F1 macro promedio: {f1_scores.mean():.4f}\")\n",
        "\n",
        "print(\"\\n****ENTRENAMIENTO FINAL****\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=21)\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n****GUARDANDO MODELO****\")\n",
        "joblib.dump(modelo, \"modelo_emociones.pkl\")\n",
        "\n",
        "print(\"\\n****EVALUACIÓN****\")\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=MAPEO_EMOCIONES.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función `predecir_emocion`:\n",
        "-Se carga el modelo previamente entrenado con **joblib.load(\"modelo_emociones.pkl\")**.\n",
        "\n",
        "-Se genera un diccionario **MAPEO_EMOCIONES_INV**, que invierte las claves y valores de **MAPEO_EMOCIONES** para poder traducir el resultado numérico de la predicción en texto.\n",
        "\n",
        "* Dentro de la función:\n",
        "\n",
        "-Se carga la imagen original sin ningún proceso, esta imagen se usa únicamente para mostrarla junto con la predicción.\n",
        "\n",
        "-Luego, se carga por segunda vez la imagen usando la función **cargar_imagen()** para procesarla.\n",
        "\n",
        "-Se recorta la imagen usando **recortar_imagen()** para convertirla en un cuadrado.\n",
        "\n",
        "-Se extraen los landmarks faciales con **extraer_landmarks()**.\n",
        "\n",
        "* Si la extracción fue exitosa, los landmarks se copian para su visualización, y luego se normalizan con **normalizar_landmarks()** para generar el vector que será usado por el modelo.\n",
        "\n",
        "-Se realiza la predicción con **modelo.predict([norm_landmarks])**, y se obtiene la etiqueta de la emoción correspondiente a través de **MAPEO_EMOCIONES_INV**.\n",
        "\n",
        "-Por último, se muestra la predicción con su imagen sin procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelo = joblib.load(\"modelo_emociones.pkl\") # carga de modelo previamente entrenado\n",
        "MAPEO_EMOCIONES_INV = {v: k for k, v in MAPEO_EMOCIONES.items()}\n",
        "\n",
        "def predecir_emocion(ruta_imagen):\n",
        "    # cargar imagen original sin procesar, es decir, tal cual como se subió \n",
        "    img_original = cv2.imread(ruta_imagen, cv2.IMREAD_COLOR)\n",
        "    if img_original is None:\n",
        "        print(\"No se pudo cargar la imagen original.\")\n",
        "        return\n",
        "\n",
        "    # cargar la versión reducida para procesamiento\n",
        "    img = cargar_imagen(ruta_imagen)  # esta sí puede ser reducida y en gris\n",
        "    if img is None:\n",
        "        print(\"No se pudo cargar la imagen para análisis.\")\n",
        "        return\n",
        "\n",
        "    img_proc = recortar_imagen(img)\n",
        "    if img_proc is None:\n",
        "        print(\"No se pudo procesar la imagen.\")\n",
        "        return\n",
        "\n",
        "    landmarks = extraer_landmarks(img_proc)\n",
        "    if landmarks is None:\n",
        "        print(\"No se detectaron landmarks.\")\n",
        "        return\n",
        "\n",
        "    landmarks_visuales = landmarks.copy()\n",
        "    norm_landmarks = normalizar_landmarks(landmarks)\n",
        "    if norm_landmarks is None:\n",
        "        print(\"No se pudo normalizar la imagen.\")\n",
        "        return\n",
        "\n",
        "    pred = modelo.predict([norm_landmarks])[0]\n",
        "    emocion = MAPEO_EMOCIONES_INV[pred]\n",
        "\n",
        "    # imagen con landmarks\n",
        "    img_color = cv2.cvtColor(img_proc.copy(), cv2.COLOR_GRAY2BGR)\n",
        "    h, w = img_proc.shape[:2]\n",
        "\n",
        "    for punto in landmarks_visuales:\n",
        "        x = int(punto[0] * w)\n",
        "        y = int(punto[1] * h)\n",
        "        if 0 <= x < w and 0 <= y < h:\n",
        "            cv2.circle(img_color, (x, y), 2, (0, 255, 0), -1)\n",
        "\n",
        "    img_original_rgb = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
        "    img_con_landmarks = cv2.cvtColor(img_color.copy(), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # mostrar la imagen sin procesar y la imagen con landmarks\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_original_rgb)\n",
        "    plt.title(\"Original (sin procesar)\")\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img_con_landmarks)\n",
        "    plt.title(f\"Predicción: {emocion}\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pruebas\n",
        "predecir_emocion(\"pruebas/p1.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predecir_emocion(\"pruebas/p2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predecir_emocion(\"pruebas/p3.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predecir_emocion(\"pruebas/p4.jpg\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predecir_emocion(\"pruebas/p5.jpg\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predecir_emocion(\"pruebas/p6.jpg\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
